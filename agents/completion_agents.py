from agents.retrieval_context import retrieve_legal_text
from agents.LLM import call_llm_with_citation_test
from agents.evaluation_agent import process_evaluation
import json

def retrieval_agent(state):
    """
    Fetches relevant legal texts from ChromaDB.
    """
    print(f"ğŸ”¹ Retrieval Agent Fetching Text for: {state.query} ({state.law_type})")

    # âœ… Call Retrieval Logic
    retrieved_texts = retrieve_legal_text(state.query, state.law_type)

    # âœ… Debugging Print
    print(f"DEBUG: Retrieved Texts Before Storing â†’ {retrieved_texts}")

    # âœ… Ensure Retrieved Texts are Stored in Correct State Key
    if isinstance(retrieved_texts, list) and all(isinstance(text, dict) for text in retrieved_texts):
        state.retrieved_texts = retrieved_texts  # âœ… Correct Storage
    else:
        print("âš ï¸ Warning: Retrieved texts are not in a valid format! Resetting to empty.")
        state.retrieved_texts = []

    print(f"âœ… Stored Retrieved Texts: {len(state.retrieved_texts)} â†’ {state.retrieved_texts}")
    
    return state




def llm_agent(state):
    """
    Generates a response using retrieved legal text.
    """
    print(f"ğŸ”¹ LLM Agent Generating Response for: {state.query}")

    # âœ… Ensure Retrieved Texts Exist Before Calling LLM
    if not state.retrieved_texts or not isinstance(state.retrieved_texts, list):
        print("âš ï¸ Warning: No retrieved texts available! LLM cannot generate response.")
        state.llm_responses = []  # âœ… Prevents errors in evaluation
        return state

    # âœ… Extract Only the Legal Texts
    retrieved_texts_str = " ".join([text.get("text", "") for text in state.retrieved_texts])

    # âœ… Call LLM and Ensure Proper Response Format
    llm_responses = call_llm_with_citation_test(state.query, retrieved_texts_str)

    # âœ… Debugging Print
    print(f"DEBUG: LLM Responses Before Storing â†’ {llm_responses}")

    # âœ… Ensure LLM Response is a List of Dictionaries
    if isinstance(llm_responses, list) and all(isinstance(resp, dict) for resp in llm_responses):
        state.llm_responses = llm_responses  # âœ… Store properly structured responses
    else:
        print("âš ï¸ Warning: LLM response is not a valid JSON list of dictionaries! Resetting to empty.")
        state.llm_responses = []  # âœ… Prevents errors

    print(f"âœ… Stored LLM Responses: {len(state.llm_responses)} â†’ {state.llm_responses}")
    
    return state








def evaluation_agent(state):
    """
    Evaluates the LLM response using NLP metrics and selects top responses.
    """
    print(f"ğŸ”¹ Evaluation Agent Scoring Response for LLM Output.")

    # âœ… Debugging Prints
    print("DEBUG: Retrieved Texts â†’", state.retrieved_texts)
    print("DEBUG: LLM Responses â†’", state.llm_responses)

    # âœ… Ensure LLM Responses are in the Correct Format
    if not isinstance(state.llm_responses, list) or not all(isinstance(resp, dict) for resp in state.llm_responses):
        print("âš ï¸ Error: LLM responses are not in the correct format!")
        state.top_responses = []
        state.clubbed_reference_text = ""
        return state  # âœ… Return early to prevent error

    # âœ… Check If Retrieved Texts Are Empty
    if not state.retrieved_texts:
        print("âš ï¸ Warning: Retrieved texts are empty! Evaluation cannot proceed.")
        state.top_responses = []
        state.clubbed_reference_text = ""
        return state  # âœ… Return early to prevent error

    # âœ… Call `process_evaluation()` and get results
    top_responses, clubbed_reference_text = process_evaluation(state.llm_responses, state.retrieved_texts)

    # âœ… Store in State
    state.top_responses = top_responses
    state.clubbed_reference_text = clubbed_reference_text

    print(f"âœ… Top Responses Stored: {len(top_responses)}")
    print(f"âœ… Clubbed Reference Text Stored")

    return state  # âœ… Return updated state





# def relevancy_agent(state):
#     """
#     Verifies the accuracy of LLM-generated legal responses.
#     """
#     print(f"ğŸ”¹ Fact-Checking Agent Verifying Response.")

#     state.fact_checked_response = verify_legal_facts(state.llm_responses, state.evaluation_scores)  # âœ… Store fact-checked response
#     return state  # âœ… Return updated state

# def responsible_ai_agent(state):
#     """
#     Applies Responsible AI checks (bias, anonymity, privacy).
#     """
#     print(f"ğŸ”¹ Responsible AI Agent Ensuring Fairness & Anonymity.")

#     state.final_response = apply_responsible_ai_checks(state.fact_checked_response)  # âœ… Store final response
#     return state  # âœ… Return updated state
